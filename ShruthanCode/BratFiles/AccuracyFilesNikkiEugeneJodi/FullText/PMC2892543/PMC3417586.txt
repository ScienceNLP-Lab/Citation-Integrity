Vitamin D deficiency among northern Native Peoples: a real or apparent problem?

Abstract

Vitamin D deficiency seems to be common among northern Native peoples, notably Inuit and Amerindians. It has usually been attributed to: (1) higher latitudes that prevent vitamin D synthesis most of the year; (2) darker skin that blocks solar UVB; and (3) fewer dietary sources of vitamin D. Although vitamin D levels are clearly lower among northern Natives, it is less clear that these lower levels indicate a deficiency. The above factors predate European contact, yet pre-Columbian skeletons show few signs of rickets—the most visible sign of vitamin D deficiency. Furthermore, because northern Natives have long inhabited high latitudes, natural selection should have progressively reduced their vitamin D requirements. There is in fact evidence that the Inuit have compensated for decreased production of vitamin D through increased conversion to its most active form and through receptors that bind more effectively. Thus, when diagnosing vitamin D deficiency in these populations, we should not use norms that were originally developed for European-descended populations who produce this vitamin more easily and have adapted accordingly.1

Vitamin D enables the human body to absorb calcium and phosphorus from food passing through the gut. It is thus essential to bone formation, as well as to proper functioning of the cardiovascular and central nervous systems. Deficiency notably results in rickets, which is an impaired mineralization of developing bone tissue. There may also be increased risk of age-related chronic diseases: osteoporosis, various cancers, diabetes, autoimmune disorders, hypertension, atherosclerosis, muscle weakness, multiple sclerosis, Alzheimer's and Parkinson's disease (1–3).

Excessive intake of vitamin D, however, likewise leads to adverse health outcomes, often the same ones that result from insufficient intake. Disease risk thus follows a U-shaped curve as a function of vitamin D serum levels, with optimal levels corresponding to a relatively narrow “trough.” This curve may reflect interaction between vitamin D and signalling pathways that influence growth, ageing and cancer (1,4–6).

Vitamin D has few dietary sources, the main exception being the liver oils of fatty fishes. Humans, however, can synthesize it on their own through the action of ultraviolet light (UVB wavelengths 295 to 300 nm) on sterols present in the skin. These sterols are converted to vitamin D , which in turn is converted through hydroxylation in the liver to 25(OH)D, and then in the kidneys to 1,25(OH) D, the most active form of vitamin D.

Vitamin D levels in northern Natives

Both vitamin D deficiency and rickets seem to be unusually frequent among Native peoples at northern latitudes (7–10). In Canada, a quarter of all cases involve Amerindian or Inuit children, with the highest incidences being in the Yukon Territory, the Northwest Territories and Nunavut (7). This susceptibility has been attributed to 3 predisposing factors: Northern Native peoples live at latitudes that receive too little sunlight most of the year for vitamin D synthesis in the skin.. Their skin is darker than that of Europeans and thus blocks more solar UVB.. Except for the Inuit and a few other coastal groups, they ingest very little vitamin D from dietary sources, like fatty fish. Many inland groups consume little or no fish, and most of the inland fish species are not rich in vitamin D. 

This oft-cited explanation nonetheless has several weaknesses. First, all 3 of the above factors are longstanding and predate the first contacts between northern Native peoples and Europeans, yet pre-Columbian skeletons show few signs of rickets (11). Moreover, if the problem were insufficient vitamin D for the body's requirements, natural selection should have gradually reduced these requirements in one way or another, just as it has created other adaptations to northern environments, like greater tolerance for a high-fat diet (12), faster recovery from cold stress (13,14) and less sinus exposure to cold (15).

Second, public health authorities calculate the incidence of vitamin D deficiency rickets by excluding those cases that have dietary causes (7). But dietary causes are ubiquitous. Northern Natives eat much less meat now than in the past, and a high meat diet seems to reduce the risk of rickets independently of vitamin D intake (16). Modern diets also contain substances that react with calcium or phosphorus to form insoluble salts, thereby depleting the body's supply of usable calcium and phosphorus. Such substances include phytic acids in commercially processed cereals, sodium bicarbonate in baking soda, and aluminum hydroxide in antacids. Cereal induced rickets has been reported by several authors (17–20). In particular, unleavened bread and chapatti have been blamed for the relatively high incidence of rickets in the Middle East and South Asia (21,22). Antacid-induced rickets has likewise been widely reported (23–28). It can even result from typical levels of antacid intake: 42 to 48 mL of Maalox per day over 5 to 6 weeks in the case of a 3-month-old, and 4 to 8 teaspoons of Mylanta per day over 5 years in the case of a young man (25,26). Given the differences in calcium metabolism between Inuit and Europeans (29), northern Natives may be more vulnerable to these calcium and phosphorus precipitating substances.

Third, when northern Natives develop rickets, they are routinely diagnosed as having vitamin D deficiency if their serum 25(OH)D level is low. Yet this level is often low even in apparently healthy Inuit and Amerindians (8,9,29). In Manitoba, 32% of rural Amerindian women and 30.4% of urban Amerindian women have serum 25(OH)D levels of less than 37.5 nmol/L (9). Such levels are well below the current recommended minimum of 75 nmol/L. Thus, among northern peoples, the purported association between rickets and low vitamin D levels may be more apparent than real.

Indeed, vitamin D levels are generally low in darker-skinned human populations, even those that still inhabit the tropical zone and receive intense sunlight. Among young, tanned Hawaiians with 22.4 hours per week of unprotected sun exposure, 51% were found to have serum 25(OH)D below 75 nmol/L (30). A study from south India found levels below 50 nmol/L in 44% of the men and in 70% of the women. The subjects were “agricultural workers starting their day at 0800 and working outdoors until 1700 with their face, chest, back, legs, arms, and forearms exposed to sunlight” (22). In a study from Saudi Arabia, levels were below 25 nmol/L in 35%, 45%, 53% and 50% of normal male university students of Saudi, Jordanian, Egyptian and other origins, respectively (31). In a sample of healthy Middle Eastern athletes, 91% had levels below 50 nmol/L (32). There is significant variation even among Europeans, with levels being lower in central and southern Europeans than in lighter-skinned Swedes (33). Finally, a meta analysis concluded that serum 25(OH)D levels are significantly lower in people of non-European origin than in people of European origin. In the first group, the levels are consistently low regardless of latitude (34).

Adaptations to low vitamin D levels

Darker-skinned humans seem to cope with low levels of vitamin D by using this vitamin more efficiently or by increasing calcium and phosphorus absorption via other means (35).

Thus, a single UVB exposure produces less vitamin D  in black subjects than in whites. The difference narrows, however, after liver hydroxylation to 25(OH)D, and disappears after kidney hydroxylation to 1,25(OH) D. The most active form of vitamin D is thereby kept at a constant level regardless of skin colour (36–38). This may be why nearly half of African Americans are classified as vitamin D deficient and yet few show signs of calcium deficiency. In fact, this population has less osteoporosis, fewer fractures and a higher bone mineral density than do Euro-Americans, who generally produce and ingest more vitamin D (39). The same apparent contradiction emerges from a survey of East African immigrant children in Australia. None had rickets despite very low serum 25(OH)D, with 87% of them having less than 50 nmol/L and 44% having less than 25 nmol/L (40).

These levels are apparently lower for genetic reasons. A study of African Americans found that serum 25(OH)D decreased linearly with increasing African ancestry, the decrease being 2.5–2.75 nmol/L per 10% increase in African ancestry. The study also found that sunlight and diet were 46% less effective in raising these levels among subjects with high African ancestry than among those with low/medium African ancestry (41).

Low vitamin D levels have led to similar adaptations in other darker-skinned populations. The Inuit have normal serum calcium despite low serum 25(OH)D and a calcium-deficient diet. They seem to absorb calcium more efficiently, perhaps because of receptors that bind more strongly to the vitamin D molecule (42). They also have above-normal levels of 1,25(OH) D despite having low levels of 25(OH)D. Thus, in Inuit peoples, vitamin D is produced at a lower rate but is then converted at a higher rate to its most active form (29). Perhaps for similar reasons, Amerindian women have lower serum 25(OH)D than do European American women and yet possess a higher bone mass until menopause (9,43).

In sum, there are many possible reasons why some human populations have managed to survive and even thrive despite apparently deficient levels of vitamin D. This vitamin may be less necessary because stores of calcium and phosphorus are used more efficiently, because these elements are absorbed from the gut via alternate metabolic pathways, because vitamin D is transported more efficiently through the bloodstream and into target tissues, because the vitamin D receptor binds more strongly to this molecule, or because 25(OH)D is converted to 1,25(OH) D at a higher rate (6,44).

Ethnic differences in vitamin D toxicity

If the optimal range of 25(OH)D is lower in darker-skinned peoples, the upper boundary where toxicity begins will correspondingly be lower, perhaps lower than current recommended levels.

This toxicity threshold is still debated even with regard to European populations, for whom we have more data. It may be that this threshold differs from one disease risk to the next, although the differences in the literature could also reflect differences in methodology or study population. In particular, most studies poorly control for ethnic origin. Participants are often described as “white” or “Caucasian,” yet vitamin D metabolism may significantly differ between lighter-skinned northern Europeans and darker skinned southern Europeans (33).

Nonetheless, there is broad agreement that a U-shaped response curve describes the relationship between serum 25(OH)D and various disease risks (45). According to a Finnish study, the risk of prostate cancer increases below 40 nmol/L and above 60 nmol/L (1,4). In women from the United States, Finland and China, mortality for 7 types of cancer (endometrial, esophageal, gastric, kidney, non-Hodgkin's lymphoma, pancreatic, ovarian) increases below 45 nmol/L and above 124 nmol/L (46). Another transnational study reported that the risk of pancreatic cancer is higher above 100 nmol/L (47). The Framingham Heart Study concluded that cardiovascular disease risk increases below 50 nmol/L and above 62.5 nmol/L, while the NHANES III found higher all-cause mortality above 122.5 nmol/L (48). Perhaps most worri some, animal and human studies have indicated a U-shaped response curve for lifespan, with premature ageing associated with both too little and too much vitamin D (1,4).

These response curves generally bottom out at a lower range of 25(OH)D levels when the population is of non-European descent. For risk of pancreatic cancer, the optimal range seems to be lower among Chinese than among European Americans (47). For risk of atherosclerosis, the optimal range is likewise lower among African Americans than among European Americans. A sample of African Americans showed a positive correlation between calcified plaque formation and serum 25(OH)D (mean=50.4±30.5 nmol/L), despite a negative correlation among European Americans over the same range (49). Similarly, vitamin D supplementation did not benefit a sample of postmenopausal African-American women in terms of less bone loss or more bone turnover, even though mean serum 25(OH)D was only 47 nmol/L before supplementation (50).

Northern Native peoples may likewise respond negatively to vitamin D supplementation, just as Inuit children respond negatively to calcium supplementation despite an apparently calcium-deficient diet (42).

Conclusion

Much work remains to be done to understand the differences in vitamin D metabolism among human populations. Clearly, humans have successfully adapted to environments where vitamin D is much less available through solar UVB synthesis in the skin or through dietary intake. Such adaptation has probably come about through a number of physiological changes, given the complexity of vitamin D metabolism and the possibilities for alternate metabolic pathways.

There is thus good reason to believe that we are pathologizing northern Natives who, in fact, are quite healthy. Our vitamin D norms may simply reflect what is normal for humans whose physiology has adapted to lighter skin, lower latitudes and more solar UVB.

For the time being, it would be unwise to supplement the diets of northern Native peoples with vitamin D in the hope of lowering disease risk. The actual health outcomes might not be as expected. In particular, if lower serum 25(OH)D levels simply reflect a lower optimal range, dietary supplementation might push treated individuals into a zone of suboptimal or even adverse outcomes.

